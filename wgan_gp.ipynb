{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wgan_gp",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz6M9mcQSXgv",
        "colab_type": "text"
      },
      "source": [
        "![임도형 커멘트](https://github.com/dhrim/keras_example_seminia_2020/raw/master/comment.png)\n",
        "\n",
        "# 개요\n",
        "\n",
        "- 원 본 : https://keras.io/examples/generative/wgan_gp/\n",
        "- 작업 : fashion MNIST를 WGAN-GP로 학습\n",
        "- 데이터 : fashion MNIST\n",
        "- 적용 모델 : WGAN-GP\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "# 데이터\n",
        "\n",
        "fashion MNIST\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "# WGAN-GP\n",
        "\n",
        "학습이 잘 되도록 vanila GAN의 로스를 변경한 것.\n",
        "Wastertein loss를 사용하고, grandient에 패널티를 포함했다.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "# 특이 사항\n",
        "\n",
        "fashion MNIST는 28x28 크기이다.\n",
        "\n",
        "2의 배수로 처리하려면 32x32크기로 하는게 편한다.\n",
        "\n",
        "이를 위해 ZeroPadding2D과 Cropping2D를 사용하였다.\n",
        "\n",
        "```\n",
        "Discriminator에서 28x28 입력을 받고서 이를 32x32로\n",
        "x = layers.ZeroPadding2D((2, 2))(img_input)\n",
        "\n",
        "Generator에서 32x32로 생성하고 이를 28x28로 \n",
        "x = layers.Cropping2D((2, 2))(x)\n",
        "```\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "# Model.train_step() 변경\n",
        "\n",
        "일반적으로 model.fit()을 호출하면 Keras는 내부적으로 model.train_step()을 호출한다.\n",
        "\n",
        "여기에서는 train_step()을 다시 정의하여 사용하였다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whs8bCzFSpou",
        "colab_type": "text"
      },
      "source": [
        "# 태그\n",
        "```\n",
        "#fashion_mnist\n",
        "#wgan-gp\n",
        "#save_image_on_epoch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KvEJdovDRyhz"
      },
      "source": [
        "# WGAN-GP overriding `Model.train_step`\n",
        "\n",
        "**Author:** [A_K_Nain](https://twitter.com/A_K_Nain)<br>\n",
        "**Date created:** 2020/05/9<br>\n",
        "**Last modified:** 2020/05/9<br>\n",
        "**Description:** Implementation of Wasserstein GAN with Gradient Penalty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-vHlaEkhRyh1"
      },
      "source": [
        "## Wasserstein GAN (WGAN) with Gradient Penalty (GP)\n",
        "\n",
        "The original [Wasserstein GAN](https://arxiv.org/abs/1701.07875) leverages\n",
        "the Wasserstein distance to produce a value function that has better theoretical\n",
        "properties than the value function used in the original GAN paper. WGAN requires that\n",
        "the discriminator (aka the critic) lie within the space of 1-Lipschitz functions.\n",
        "The authors proposed the idea of weight clipping to achieve this constraint. Though\n",
        "weight clipping works, it can be a problematic way to enforce 1-Lipschitz constraint\n",
        "and can cause undesirable behavior, e.g. a very deep WGAN discriminator (critic)\n",
        "often fails to converge.\n",
        "\n",
        "[WGAN-GP](https://arxiv.org/pdf/1704.00028.pdf) proposed an alternative to weight\n",
        "clipping to ensure smooth training. Instead of clipping the weights, the authors\n",
        "proposed a \"gradient penalty\": adding a loss term that keeps the L2\n",
        "norm of the discriminator gradients close to 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rqQ4J9sTRyh2"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E-wbMC1ERyh3",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NZX18zCARyh8"
      },
      "source": [
        "## Prepare Fashion-MNIST data\n",
        "\n",
        "We will be using the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset\n",
        "in this example to demonstrate the training of WGAN-GP. Each sample in this dataset is a 28x28\n",
        "grayscale image associated with a label from 10 classes (e.g. Trouser, Pullover, Sneaker, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7_NjViArRyh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "3856189c-9d67-4f6b-aeb5-573ee4291237"
      },
      "source": [
        "IMG_SHAPE = (28, 28, 1)\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# Size of noise vector\n",
        "noise_dim = 128\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "print(f\"Number of examples: {len(train_images)}\")\n",
        "print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n",
        "\n",
        "# we will reshape each sample to (28, 28, 1) and normalize the pixel values in [-1, 1].\n",
        "train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\n",
        "train_images = (train_images - 127.5) / 127.5\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Number of examples: 60000\n",
            "Shape of the images in the dataset: (28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOZaHFKiXbg0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "d3802f21-0265-4201-d57c-82f53d12ee1c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(train_images[0].squeeze(), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U6KRKyE9Ryh_"
      },
      "source": [
        "## Create the discriminator (aka critic in the original WGAN)\n",
        "\n",
        "The samples in the dataset have shape `(28, 28, 1)`. As we will be\n",
        "using strided convolutions, this can result in a shape with odd dimensions.\n",
        "For example,\n",
        "`(28, 28) -> Conv_s2 -> (14, 14) -> Conv_s2 -> (7, 7) -> Conv_s2 ->(3, 3)`.\n",
        "\n",
        "While doing upsampling in the generator, we won't get the same input shape\n",
        "as the original images if we aren't careful. To avoid this, we will do\n",
        "something much simpler. In the discriminator, we will \"zero pad\" the input\n",
        "to make the shape `(32, 32, 1)` for each sample, while in the generator we will\n",
        "crop the final output to match the shape with input shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TywL158eRyiA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "5f4eb467-870a-45c1-ccbc-3ccdab9875f1"
      },
      "source": [
        "\n",
        "def conv_block(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"same\",\n",
        "    use_bias=True,\n",
        "    use_bn=False,\n",
        "    use_dropout=False,\n",
        "    drop_value=0.5,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
        "    )(x)\n",
        "    if use_bn:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = activation(x)\n",
        "    if use_dropout:\n",
        "        x = layers.Dropout(drop_value)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_discriminator_model():\n",
        "    img_input = layers.Input(shape=IMG_SHAPE)\n",
        "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
        "    x = layers.ZeroPadding2D((2, 2))(img_input)\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        64,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        use_bias=True,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_dropout=False,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        128,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=True,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        256,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=True,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        512,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=False,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1)(x)\n",
        "\n",
        "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
        "    return d_model\n",
        "\n",
        "\n",
        "d_model = get_discriminator_model()\n",
        "d_model.summary()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 32, 32, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 16, 16, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 8, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 4,305,409\n",
            "Trainable params: 4,305,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cqAuCHPbRyiC"
      },
      "source": [
        "## Create the generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lpX_SlRuRyiD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "f9baaf9d-3035-4c8a-fb01-6279bbd403d6"
      },
      "source": [
        "\n",
        "def upsample_block(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    up_size=(2, 2),\n",
        "    padding=\"same\",\n",
        "    use_bn=False,\n",
        "    use_bias=True,\n",
        "    use_dropout=False,\n",
        "    drop_value=0.3,\n",
        "):\n",
        "    x = layers.UpSampling2D(up_size)(x)\n",
        "    x = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
        "    )(x)\n",
        "\n",
        "    if use_bn:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    if use_dropout:\n",
        "        x = layers.Dropout(drop_value)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_generator_model():\n",
        "    noise = layers.Input(shape=(noise_dim,))\n",
        "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Reshape((4, 4, 256))(x)\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        128,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        64,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
        "    )\n",
        "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
        "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
        "    x = layers.Cropping2D((2, 2))(x)\n",
        "\n",
        "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
        "    return g_model\n",
        "\n",
        "\n",
        "g_model = get_generator_model()\n",
        "g_model.summary()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              524288    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         294912    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 64)        73728     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 1)         576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 1)         4         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 1)         0         \n",
            "_________________________________________________________________\n",
            "cropping2d (Cropping2D)      (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 910,660\n",
            "Trainable params: 902,082\n",
            "Non-trainable params: 8,578\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Ple3YGyRyiF"
      },
      "source": [
        "## Create a WGAN-GP model\n",
        "\n",
        "Now that we have defined our generator and discriminator models, we will\n",
        "implement the WGAN-GP model. We will override the `train_step` for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-0th72pBRyiG",
        "colab": {}
      },
      "source": [
        "\n",
        "class WGAN(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        discriminator,\n",
        "        generator,\n",
        "        latent_dim,\n",
        "        discriminator_extra_steps=3,\n",
        "        gp_weight=10.0,\n",
        "    ):\n",
        "        super(WGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_steps = discriminator_extra_steps\n",
        "        self.gp_weight = gp_weight\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "        super(WGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "        This loss is calculated on an interpolated image\n",
        "        and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        # get the interplated image\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # 1. Get the discriminator output for this interpolated image.\n",
        "            pred = self.discriminator(interpolated, training=True)\n",
        "\n",
        "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        # 3. Calcuate the norm of the gradients\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "\n",
        "        # Get the batch size\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # For each batch, we are going to perform the\n",
        "        # following steps as laid out in the original paper.\n",
        "        # 1. Train the generator and get the generator loss\n",
        "        # 2. Train the discriminator and get the discriminator loss\n",
        "        # 3. Calculate the gradient penalty\n",
        "        # 4. Multiply this gradient penalty with a constant weight factor\n",
        "        # 5. Add gradient penalty to the discriminator loss\n",
        "        # 6. Return generator and discriminator losses as a loss dictionary.\n",
        "\n",
        "        # Train discriminator first. The original paper recommends training\n",
        "        # the discriminator for `x` more steps (typically 5) as compared to\n",
        "        # one step of the generator. Here we will train it for 3 extra steps\n",
        "        # as compared to 5 to reduce the training time.\n",
        "        for i in range(self.d_steps):\n",
        "            # Get the latent vector\n",
        "            random_latent_vectors = tf.random.normal(\n",
        "                shape=(batch_size, self.latent_dim)\n",
        "            )\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Generate fake images from the latent vector\n",
        "                fake_images = self.generator(random_latent_vectors, training=True)\n",
        "                # Get the logits for the fake images\n",
        "                fake_logits = self.discriminator(fake_images, training=True)\n",
        "                # Get the logits for real images\n",
        "                real_logits = self.discriminator(real_images, training=True)\n",
        "\n",
        "                # Calculate discriminator loss using fake and real logits\n",
        "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
        "                # Calculate the gradient penalty\n",
        "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "                # Add the gradient penalty to the original discriminator loss\n",
        "                d_loss = d_cost + gp * self.gp_weight\n",
        "\n",
        "            # Get the gradients w.r.t the discriminator loss\n",
        "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "            # Update the weights of the discriminator using the discriminator optimizer\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(d_gradient, self.discriminator.trainable_variables)\n",
        "            )\n",
        "\n",
        "        # Train the generator now.\n",
        "        # Get the latent vector\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Generate fake images using the generator\n",
        "            generated_images = self.generator(random_latent_vectors, training=True)\n",
        "            # Get the discriminator logits for fake images\n",
        "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
        "            # Calculate the generator loss\n",
        "            g_loss = self.g_loss_fn(gen_img_logits)\n",
        "\n",
        "        # Get the gradients w.r.t the generator loss\n",
        "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        # Update the weights of the generator using the generator optimizer\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(gen_gradient, self.generator.trainable_variables)\n",
        "        )\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6h8FyBQpRyiI"
      },
      "source": [
        "## Create a callback that periodically saves generated images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EN8UkbuDRyiJ",
        "colab": {}
      },
      "source": [
        "\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=6, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images = (generated_images * 127.5) + 127.5\n",
        "\n",
        "        for i in range(self.num_img):\n",
        "            img = generated_images[i].numpy()\n",
        "            img = keras.preprocessing.image.array_to_img(img)\n",
        "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "27U5STPbRyiM"
      },
      "source": [
        "## Train the end-to-end model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KjY4oj0_RyiN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "ff19a42d-6ca4-4661-dc63-05a560aeca9f"
      },
      "source": [
        "# Optimizer for both the networks\n",
        "# learning_rate=0.0002, beta_1=0.5 are recommened\n",
        "generator_optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "discriminator_optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "\n",
        "# Define the loss functions to be used for discrimiator\n",
        "# This should be (fake_loss - real_loss)\n",
        "# We will add the gradient penalty later to this loss function\n",
        "def discriminator_loss(real_img, fake_img):\n",
        "    real_loss = tf.reduce_mean(real_img)\n",
        "    fake_loss = tf.reduce_mean(fake_img)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "\n",
        "# Define the loss functions to be used for generator\n",
        "def generator_loss(fake_img):\n",
        "    return -tf.reduce_mean(fake_img)\n",
        "\n",
        "\n",
        "# Epochs to train\n",
        "epochs = 20\n",
        "\n",
        "# Callbacks\n",
        "cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n",
        "\n",
        "# Get the wgan model\n",
        "wgan = WGAN(\n",
        "    discriminator=d_model,\n",
        "    generator=g_model,\n",
        "    latent_dim=noise_dim,\n",
        "    discriminator_extra_steps=3,\n",
        ")\n",
        "\n",
        "# Compile the wgan model\n",
        "wgan.compile(\n",
        "    d_optimizer=discriminator_optimizer,\n",
        "    g_optimizer=generator_optimizer,\n",
        "    g_loss_fn=generator_loss,\n",
        "    d_loss_fn=discriminator_loss,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "wgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "118/118 [==============================] - 70s 594ms/step - d_loss: -7.7284 - g_loss: -16.3324\n",
            "Epoch 2/20\n",
            "118/118 [==============================] - 71s 599ms/step - d_loss: -7.2995 - g_loss: -3.7804\n",
            "Epoch 3/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -6.4038 - g_loss: 5.2266\n",
            "Epoch 4/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -5.7124 - g_loss: 10.0293\n",
            "Epoch 5/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -5.2442 - g_loss: 9.7953\n",
            "Epoch 6/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -4.8464 - g_loss: 10.1211\n",
            "Epoch 7/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -4.4917 - g_loss: 10.3924\n",
            "Epoch 8/20\n",
            "118/118 [==============================] - 70s 597ms/step - d_loss: -4.1899 - g_loss: 9.3160\n",
            "Epoch 9/20\n",
            "118/118 [==============================] - 70s 597ms/step - d_loss: -3.9924 - g_loss: 8.5732\n",
            "Epoch 10/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -3.7341 - g_loss: 8.1891\n",
            "Epoch 11/20\n",
            "118/118 [==============================] - 70s 597ms/step - d_loss: -3.5848 - g_loss: 9.3522\n",
            "Epoch 12/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -3.3867 - g_loss: 7.5533\n",
            "Epoch 13/20\n",
            "118/118 [==============================] - 71s 597ms/step - d_loss: -3.2659 - g_loss: 7.8738\n",
            "Epoch 14/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -3.0903 - g_loss: 6.2869\n",
            "Epoch 15/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -3.0501 - g_loss: 8.1785\n",
            "Epoch 16/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -2.8461 - g_loss: 8.6551\n",
            "Epoch 17/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -2.8691 - g_loss: 8.6503\n",
            "Epoch 18/20\n",
            "118/118 [==============================] - 70s 597ms/step - d_loss: -2.7461 - g_loss: 4.5980\n",
            "Epoch 19/20\n",
            "118/118 [==============================] - 70s 597ms/step - d_loss: -2.6859 - g_loss: 6.9392\n",
            "Epoch 20/20\n",
            "118/118 [==============================] - 71s 598ms/step - d_loss: -2.5066 - g_loss: 5.9314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7facc21cf278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BEdb4DITRyiP"
      },
      "source": [
        "Display the last generated images:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tiRVNo-hRyiP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8f598183-f6a6-4114-9037-08032e05e982"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(\"generated_img_0_19.png\"))\n",
        "display(Image(\"generated_img_1_19.png\"))\n",
        "display(Image(\"generated_img_2_19.png\"))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACyElEQVR4nAXBy29UZRQA8HPOd+bOzJ25U7hD25nOQJTw1DSkqQ1iqJoYXwsXhhAWuPUREvkD2LJxi4kaNy4NvqLRpfGVgAkVVESGFtpaoLYJnen0zvve+33n8PshCwIJWVOLI/FyzqXHWyuKiTUOSEhVFEgeun3aa8fZt7ZXnAUmC4zGWCNiLGDRH/rTr9VWLh+s3zL/RCrKiGQUlBHSjuwv/1oP5jfMaLi750hYFRyhImgKEhcc9P3lsjd+8u9vnCEQABUEIMDs7Or6g6vYvhKc2S4WlRiMA0RUAERve7q7yDcHXe/Trj8AZVAQBFWwJoNrlbu3CrdHumyraawJqRVQUVRjbKb0b0oRGDITpWDimMckiIKKhGpp/8yN1RHPHcmfutM/H51gJRRCKIxGXgi/bP0GteYanzLvfP5mr4kGFIicoaS+l6+H/v0JncpeSJJvf7aChKBKCmyP2Ve/ip665o3P/wl77vlzG1sEqEAMavnygc8eSW5XYfPq0eJG2P9rqkMGENQBZyZlYRSWH6UDf+bcodxLU+dOB5yxIoi5St9d7ODezn+T1bdfz358/I32eysJp4hIzjZfWcqObdw7vRC/vPND9W67BdFChx0BEOuR9y/VD0v1YeXO7K6zOwO4dmB9OWAWJUfe/x+N5r8erS1+8f13NzZV/S6VnwhYjQig5L2lzaGQFzQWLCAkwOW4ydlEKReaQ82h9+L1pH+pTYKKWKqaSszZ+pNLQzN21PZ//zFTe+bkbUViq3Qiv53hfmkz8mozz2EjnPWl9+V9QE1Vop/mPukx7h7vBjtXui/sqeRXvaeLjlAFAfPdsYDDTPZgvvD85L7O4eV3/1gcgKgRIG5Kq8fxTcPsNSDMTPg66KyjN0Bm4MaHQ8ddsLGRVi5sP8iVpp/9QJRIYk2TxiBHVhCVMN5xQw6qrVYx45ySYbsV6WM1sG5toQbwtQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACq0lEQVR4nAXBzWscZRgA8Od53mfe+diZTLbNbj66ESwNtT0UK8UKag/qRfDopfUoePWf8Ci9etG/wYNCEaTQUoWitAVtJf1IUkzX7CbpZLPZmZ2Z930efz+IwsSStUSIvdvfFz9dWwFgG1gOAmavggStAsB8o7/46eO/AbySR0ElVUU2GCZrN3fMjRvfntq6jqhCCGo4cCTIg2sfX/7xm3zpz1E6/q76/ciroHr2IALZVy8Pt148Hx/rhfDu5+/XzeZIAwesoBB/csWm9fXNu9XZDw6e/VF9FD24NUZERgU6/9kqPhg+2zrT28zu1YPO4L+vD255VVJVyuP7buNVgr3F2ZNd+WcU/EVXLRKyERrcPErOFV+Mo+DqKbw4fe/p66Wnb0VNq+xU5uHhuf2tQfLly2Aj591Lo7YzvtwdOmUxeDZa2n+0sD3eLWNpwnL/Unq/GKVCykzYPCrx7cViPR4189IvHx6f6e089KLK3tP2z1ea2BwFvrboIlPlyZ2h3xMAJqDqlxfnybU7lvPqpF7u4mk3Gs4QgJGgnh3Yd/siPvfdAiwVK8l25VCAnQGVI1sUXZy6XKd1009nk1lDioZURKqTw72gisM+QBJnk/lB0MlCBWEySkkWY7UOw5bsWt2EqaxxaQSQAQHMCa8vhBCtOp5j3Z287jICIDKIooqoMHUC4RRNvtO+keAeojIgKbi6lzOEFNvSebTp82pSASCDomq0esHNoJcGJBzm+bKstMc1EIEIOpfPM8prgxAFJDKZIAgAEBCiYBl3qbOWqLS8YPNgmu63KsACmmm0cBJ1IseZiSuKUjRpowjCCOTdSkgBadpgXdtGxvywVAUhNl5rCKPf3slC4xJdVMQP/a9DUlJlNQL+yd6/p/tvLjexr2FavHr8Q1MqIf4PlIVhGgjLCEcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACfElEQVR4nCXETWtcZRgG4Pt+3ue8ZzIfyUwZU5NJS2mkpdAWWkGoCxG0iy6k6KJCxB8hiD9AcOHGvRvrRl10lVURd4K48oNSsCBCW7WMk0lsepqTOXPe93lceC0uFoA7HISDJD0JQBHXBTW2mQju5gKHOZgBRNiSmQonCCO1Id0Bd4DCElnFAAKUnOnGkV85/WNdVd5PK5UanCEFzSk7RPs3PjxTo/pm9mg+W6IotNAQo4qETufs1/PKpnvpyU8722tdcXNCSFVYCBuXOgVODqX/z1tlm5VuAJNRxFLYnqB5OmbV3v9t1luKM8ADQDMiXSS1bLMUN2/F563QoAQYAixbV2CDYCXS+XeiqMGN5p5BzTIs47Ke1Zt58DAOp0KhwxDoll1i61k2XrSVcXFrDHF3msNdkLn2Zi/PH6VS5Khz7lNXgmLisAz4J+XxcTvohD3lRC+vqAnM3QXg1e3roSnK7gKL/hJHsqXIwQFPhH90o6Y2FkNegS5RnBKlkRlu8e1LGMy9p79DY1miXD2tLskAp955vc5Yzd1io3BNaTZcvSdA1BjLzduvHfxwkIre4aFQioWf6JA63Hrl3V8e965P8vqV0UDcn8SF5kVo6hMbau+/125Jr2t71UTtuCmv6V/fv9rvyZ8PqC+fm9Wje+ubw1Ntg2e9qA9/vr9728J+H7XsXK5k/7Ef7s7rp9aP1cHux6PPX5qsnhmv/cvxzgtvXF30OYvdkPcffNn8MT174bNI+/a7rxg6OHnz2Xq4Oyr3ptPkDoh88OvxtS/yEdXd4QT+TyCSeP7vplMRChpA0gLgcBSZzv3i+RKuLCQRzpAlO+EgQkbhOcD1P3jQOtlRtLTdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DATscxtJXfUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "cbc2122e-2457-4c2a-b54e-f81d080aa97e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "\n",
        "\n",
        "img = image.imread('generated_img_0_19.png')\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "img = image.imread('generated_img_1_19.png')\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "img = image.imread('generated_img_2_19.png')\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}